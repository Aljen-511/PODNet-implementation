import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Subset, DataLoader, ConcatDataset
from sklearn.cluster import KMeans

# ä¸ºäº†èƒ½é¡ºåˆ©å¯¼å…¥dataprocessæ¨¡å—
import sys
import os
cur_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(cur_dir, ".."))
from dataprocess import process
from model.PODNet import integratedMdl
from test.test import acc_loop
from model.utils import herdingStrategy
import numpy as np
import copy


# ç»˜åˆ¶è¿›åº¦æ¡
from tqdm import tqdm

# è¿™é‡Œè¦å®Œæˆè®­ç»ƒä»»åŠ¡ï¼Œé¦–å…ˆè¦å®Œæˆä¸€ä¸ªåŸºæœ¬çš„æ¥å£å‡½æ•°ï¼štrain
# è¯¥æ¥å£å‡½æ•°æ¥æ”¶å‚æ•°ï¼Œå¹¶è°ƒç”¨è¯¸å¤šloopå‡½æ•°

# é‰´äºä¹‹å‰çš„ç»éªŒï¼Œè¿™é‡Œå†³å®šåœ¨è®­ç»ƒå–æ‰¹æ¬¡æ•°æ®çš„æ—¶å€™å†æŠŠæ•°æ®è½¬åˆ°gpu


# éœ€è¦è§„å®šï¼š
# on_gpu, batch_size, inc_step, data_name, train_with_pretrained_inc, load_pretrained_path, save_pretrained_path 
# SGD_learning_rate, decay_rate, SGD_momentum, train_ratio, SGD_momentum, T_max(ä½™å¼¦é€€ç«å‘¨æœŸ), max_epoch
def train_and_evaluate(**kargs):
    #  éœ€è¦å‚è€ƒåŸåšæ³•ï¼Œç»“åˆcosine annealing scheduling
    # é¦–å…ˆè¿›è¡Œå‚æ•°è§£æ
    # è¿™äº›äº¤ç»™ä¸»å‡½æ•°è°ƒç”¨çš„æ—¶å€™å¤„ç†å³å¯
    ######################################
    torch.autograd.set_detect_anomaly(True)
    ######################################
    data_name = kargs["data_name"] if "data_name" in kargs else "cifar100"

    

    print(f"Loading data from {data_name}......")
    base_trainset = process.generalDataSet(data_name, "train") if "train_ratio" not in kargs else \
                   process.generalDataSet(data_name, "train", train_ratio = kargs["train_ratio"])
    ##########################
    kargs["train_ratio"] = 0.9
    ##########################
    base_valset = process.generalDataSet(data_name, "val") if "train_ratio" not in kargs else \
                  process.generalDataSet(data_name, "val",train_ratio = kargs["train_ratio"])

    base_testset = process.generalDataSet(data_name, "test")

    print("Data Loading done! Initializing the neccessary configurations for training......")

    
    # é¦–å…ˆæ ¹æ®ä¿¡æ¯å®šä¹‰æ¨¡å‹ï¼Œå¹¶å°½å¯èƒ½åœ¨åˆå§‹åŒ–æ—¶å°±å°†æ¨¡å‹è½¬ç§»åˆ°gpuä¸Š
    model = integratedMdl()
    accuracy_metric = []
    # å¼€å§‹åŸºç¡€ä»»åŠ¡çš„è®­ç»ƒ
    learned_classes = 0 #å·²ç»å­¦è¿‡çš„ç±»åˆ«æ•°ç›®
    # è¿›å…¥é¦–æ¬¡è®­ç»ƒå‰ï¼Œå…ˆè¿›è¡Œproxyçš„åˆå§‹åŒ–ï¼Œè°ƒç”¨pretrain
    
    print("Start the basic training(50 classes)...")
    pre_train(model,[i for i in range(50)],base_trainset)
    basic_loop(model=model,
               lr_=kargs["SGD_learning_rate"],
               weight_dec=kargs["decay_rate"],
               momentum_=kargs["SGD_momentum"],
               baseTrainset=base_trainset,
               baseValset=base_valset,
               T_max_ = kargs["T_max"],
               max_epoch=kargs["max_epoch"],
               batch_size_=kargs["batch_size"])
    after_train(model, base_trainset,[i for i in range(50)] )
    accuracy_metric.append(acc_loop(model,base_testset,50))
    learned_classes += 50
    print(f"Basic traing is done. Average accuracy on testset is : {accuracy_metric[-1]:.6f}")
    
    
    inc_step = kargs["inc_step"]
    num_tasks = int(np.ceil(50/inc_step))
    print(f"Start the increase training.\nincrease step: {inc_step}\ttasks number: {num_tasks}")
    for task in range(num_tasks):
        # TODO: å°†ä¹‹å‰ä¿ç•™çš„æ—§æ ·æœ¬å¹¶å…¥è®­ç»ƒé›†
        # å¼€å§‹æ¯ä¸ªå¢é‡é˜¶æ®µçš„è®­ç»ƒ
        print(f"task {task}")
        pre_train(model, [i for i in range(learned_classes, learned_classes+inc_step)], base_trainset)
        basic_loop(lr_=kargs["SGD_learning_rate"],
                   weight_dec=kargs["decay_rate"],
                   momentum_=kargs["SGD_momentum"],
                   baseTrainset=base_trainset,
                   baseValset=base_valset,
                   T_max_=kargs["T_max"],
                   max_epoch=kargs["max_epoch"],
                   batch_size_=kargs["batch_size"],
                   inc_classesLst=[i for i in range(learned_classes, learned_classes+inc_step)])
        after_train(model=model, 
                    baseTrainset=base_trainset, 
                    new_classLst=[i for i in range(learned_classes, learned_classes+inc_step)])
        accuracy_metric.append(acc_loop(model=model,baseTestset=base_testset, inc_step=inc_step))
        learned_classes += inc_step
        cur_avg_acc = np.mean(np.array(accuracy_metric))
        print(f"Current average class accuracy: {cur_avg_acc:.6f}")
    
    #å®Œæˆå¢é‡å­¦ä¹ ï¼Œå¼€å§‹æ‰“å°è¯„ä¼°ä¿¡æ¯
    

    
    ##### TODO: 1. å®ç°ä¸¤ç§å¾ªç¯(ä¸€ç§ç»™åˆæ¬¡è®­ç»ƒä»»åŠ¡ï¼Œä¸€ç§ç»™å¢é‡è®­ç»ƒä»»åŠ¡) /6.1 âˆš
    #           2. å®ç°æŒ‡æ ‡(å¹³å‡ç±»åˆ«å‡†ç¡®ç‡) /6.1 âˆš
    #           3. å®ç°testçš„åŠŸèƒ½ï¼Œå› ä¸ºç°åœ¨æ¯ä¸€æ¬¡å¢é‡ä»»åŠ¡ä¹‹åéƒ½è¦è®¡ç®—å‡†ç¡®ç‡ï¼Œæ‰€ä»¥éœ€è¦å†™testçš„å¾ªç¯ /6.1 âˆš
    #           4. è¿›åº¦æ¡ /6.1 âˆš

    #metricå…ˆä¸è¦ç‹¬ç«‹å‡ºæ¥ï¼Œå› ä¸ºæ²¡æœ‰å¿…è¦
    


    pass
    

# åœ¨çœŸæ­£çš„è®­ç»ƒå¼€å§‹å‰è¿è¡Œï¼Œåˆå§‹åŒ–proxys
# pythoné»˜è®¤æŒ‰å¼•ç”¨ä¼ å€¼
def pre_train(model: integratedMdl, newClassLst: list, baseData: process.generalDataSet):
    '''
    model: è¿˜æ²¡åˆå§‹åŒ–æ–°ç±»åˆ«proxyçš„æ¨¡å‹(integratedMdlç±»å‹)
    newClassLst: æ–°å¢ç±»åˆ«åˆ—è¡¨ï¼Œä¾‹å¦‚[0,1,2], å³æ–°å¢ç±»åˆ«0,1,2(æŒ‰é¡ºåº,æŒ‡çš„å¹¶ä¸æ˜¯å®é™…ç±»åˆ«æ ‡ç­¾)
    baseData: å­˜å‚¨æ•´ä¸ªè®­ç»ƒé›†çš„åŸºç¡€æ•°æ®é›†(generalDataSetç±»å‹)
    '''
    # - è¿™é‡Œåªéœ€è¦æ‰§è¡Œæ¨ç†ï¼Œä½†æœ€å¥½ï¼Œåœ¨è¿™é‡Œå°±æŠŠæ•°æ®éƒ½è½¬ç§»åˆ°gpuä¸Š
    #   è¿™æ ·ï¼Œæœ€åå¾—åˆ°çš„proxyå¼ é‡ä¹Ÿéƒ½ä¼šåœ¨gpuä¸Š
    # - è¿™é‡Œçš„modelå·²ç»è½¬ç§»åˆ°gpuäº†(è‹¥å¯ç”¨)
    new_proxys = {}
    batch_size = 128
    clusterer = KMeans(n_clusters=model.curModel.proxyNums,random_state=42)

    # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œæ¨ç†ï¼Œå¾—åˆ°æ¯ä¸ªç±»åˆ«çš„featureçš„kä¸ªèšç±»ä¸­å¿ƒï¼Œä½œä¸ºåˆå§‹åŒ–çš„proxy
    for newClass in newClassLst:
        subset = baseData.getSpecificData(newClass)
        loader = DataLoader(subset, batch_size=batch_size, shuffle=False)
        # å¼€å§‹è¿›è¡Œæ¨ç†ï¼Œå¹¶è·å–å…¶è¾“å‡º
        feas = []
        with torch.no_grad():
            for images , class_ in loader:
                images = images.to("cuda" if torch.cuda.is_available() else "cpu")
                output = model.curModel(images)

                # outputå½¢çŠ¶(batchsize, feas, 1)
                # Kmeansæ¥å£, æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾ï¼Œæ‰€ä»¥åº”è¯¥æŠŠå®ƒè½¬æˆ(batchsize, feas)
                output = torch.squeeze(output).to("cpu")
                feas.append(output)

            forCluser = torch.cat(feas, dim=0)
            clusterer.fit(forCluser)
            # è¿™é‡Œçš„proxyå½¢çŠ¶è¿˜éœ€è¦å†å•†æ¦·
            # è¿™é‡Œä¸€å®šè¦ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat, å¦åˆ™åç»­æ— æ³•å’Œç¥ç»ç½‘ç»œçš„è¾“å‡ºåšè¿ç®—
            proxys = torch.tensor(clusterer.cluster_centers_,dtype=torch.float32).to("cuda" if torch.cuda.is_available() else "cpu")
            # æˆ‘è¿™é‡Œçš„proxyï¼Œå½¢çŠ¶ä¸€å®šè¦æ˜¯(10, feas, 1)ï¼Œæœ€åä¸€ç»´ä¸€å®šæ˜¯1ï¼
            proxys.unsqueeze(-1)
            # è¿™é‡Œdebugæ—¶æ’æŸ¥å‡ºé—®é¢˜æ‰€åœ¨, class_æ˜¯ä¸€ä¸ªtensor, ä¸èƒ½æ‹¿tensorä½œä¸ºå­—å…¸çš„é”®, å¦åˆ™åç»­æ— æ³•ç´¢å¼•
            # new_proxys[class_] = proxys #æ—§å†™æ³•
            # ç¡®ä¿ç´¢å¼•ä¸ºæ•´æ•°
            new_proxys[int(class_[0].item())] = proxys
            
    model.curModel.append_proxys(new_proxys)

# åœ¨è®­ç»ƒå®Œæˆä¹‹åï¼Œåœ¨æ¨¡å‹ä¸­ä¿ç•™ä¸€å®šçš„æ—§ç±»åˆ«æ ·æœ¬
def after_train(model: integratedMdl, 
                baseTrainset:process.generalDataSet,
                new_classLst
                ):
    '''
    modelæ˜¯å·²ç»ç»è¿‡è®­ç»ƒä¹‹åçš„æ¨¡å‹äº†
    new_classLstæ˜¯æœ¬æ¬¡å¢é‡å­¦ä¹ çš„ç±»åˆ«åºå·
    baseTrainsetå…·æœ‰æ‰€æœ‰ç±»åˆ«çš„è®­ç»ƒæ•°æ®
    '''
    batch_size_ = 128
    num_workers_ = 2
    # TODO: è°ƒç”¨æ ·æœ¬é›†ç®¡ç†ç­–ç•¥ï¼Œä¿ç•™æŸäº›æ—§æ ·æœ¬ âˆš
    # æ‰€ä»¥è¿™é‡Œéœ€è¦åœ¨å·²ç»è®­ç»ƒå®Œçš„æ¨¡å‹ä¸Šè¿›è¡Œæ¨ç†ï¼Œå¾—åˆ°ç‰¹å¾ï¼Œä¹‹åå†è°ƒç”¨
    # modelé‡Œçš„samplemanagerï¼Œè¿ç”¨herdingç­–ç•¥ä¿ç•™æ ·æœ¬
    for new_class in new_classLst:
        subset = baseTrainset.getSpecificData(new_class)
        dataloader = DataLoader(subset, batch_size=batch_size_, num_workers=num_workers_, shuffle=False)
        with torch.no_grad():
            features = []
            for image, _ in dataloader:
                image = image.to("cuda" if torch.cuda.is_available() else "cpu")
                output = model.curModel(image)
                # è¿™é‡Œè½¬ç§»å›cpuæ˜¯ä¸ºäº†åç»­æ“ä½œçš„ç»Ÿä¸€
                output = torch.squeeze(output).to("cpu")
                features.append(output)
            samples = torch.cat(features,dim=0).squeeze()
            chosen_idx = herdingStrategy(samples)
            model.setManager.appendSamples(Subset(subset, chosen_idx))  

    pass


# è§„å®šæ¨¡å‹çš„åŸºæœ¬å¾ªç¯
def basic_loop(model:integratedMdl,
               lr_, weight_dec, momentum_, 
               baseTrainset:process.generalDataSet, 
               baseValset:process.generalDataSet, 
               T_max_, max_epoch, batch_size_, 
               inc_classesLst = None
               ):
    '''
    model: å¾…è®­ç»ƒçš„æ¨¡å‹
    lr_: SGDä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
    weight_dec: SGDä¼˜åŒ–å™¨çš„è¡°å‡æƒé‡
    baseTrainset: åŸºæœ¬è®­ç»ƒé›†
    baseValset: åŸºæœ¬éªŒè¯é›†
    T_max: ä½™å¼¦é€€ç«ç­–ç•¥çš„å‘¨æœŸ
    max_epoch: æœ€å¤šçš„è®­ç»ƒä»£æ•°
    batch_size: è®­ç»ƒçš„æ‰¹æ•°
    inc_classesLst: å½“å‰å¢é‡å­¦ä¹ çš„ç±»åˆ«åºå·åˆ—è¡¨, é•¿åº¦å³ä¸ºinc_step,
                    é»˜è®¤ä¸ºNoneæ—¶, è¯´æ˜è¿™æ˜¯ä¸€ä¸ªbaseè®­ç»ƒå¾ªç¯
    '''
    # TODO: 
    #   1. æ—©åœç­–ç•¥ âˆš
    #   2. ä½™å¼¦é€€ç«ç­–ç•¥ âˆš

     
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    num_workers_ = 4
    
    # è¿™é‡Œä¸è¦å¿˜äº†, è¦å°†ä¹‹å‰ä¿ç•™çš„æ—§ç±»åˆ«æ ·æœ¬åˆå¹¶è¿›æ¥
    if inc_classesLst is None:
        basic_task_set = baseTrainset.getIncData4Train(base = True)
    else:
        basic_task_set = ConcatDataset([baseTrainset.getIncData4Train(inc_nums=len(inc_classesLst)),
                                        model.setManager.dataset])
    
    
    basic_val_set = baseValset.getIncData4Train(base=True) if inc_classesLst is None else \
                    baseValset.getIncData4Train(inc_nums=len(inc_classesLst))
    
    train_dataloader = DataLoader(basic_task_set,shuffle=True, batch_size=batch_size_,num_workers=num_workers_)
    val_dataloader = DataLoader(basic_val_set, shuffle=False, batch_size=batch_size_, num_workers=num_workers_)
    # åˆ›å»ºä¼˜åŒ–å™¨ï¼Œå¹¶ç»‘å®šä½™å¼¦é€€ç«å­¦ä¹ ç­–ç•¥
    optimizer = torch.optim.SGD(lr=lr_, weight_decay=float(weight_dec), momentum=momentum_, params=model.curModel.parameters())
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=T_max_)
    # !è¿™é‡Œå¯èƒ½å­˜åœ¨ä¸€äº›é—®é¢˜...
    best_acc = -1.0
    for epoch in range(max_epoch):
    # for epoch in tqdm(range(max_epoch),desc=f"Epoch nums",unit="epoch",leave=False):
        # for image, label in train_dataloader:
        for image, label in tqdm(train_dataloader, desc=f"training Epoch {epoch+1}/{max_epoch}",unit="itr",leave=False):
            # è¿™é‡Œå°½å¯èƒ½ä½¿å…¶è½¬ç§»åˆ°gpuä¸Š
            image = image.to("cuda" if torch.cuda.is_available() else "cpu")
            label = label.to("cuda" if torch.cuda.is_available() else "cpu")
            optimizer.zero_grad()
            y_output = model.curModel(image)
            # è‹¥æ˜¯å¢é‡å­¦ä¹ ä»»åŠ¡ï¼Œåˆ™è¿˜éœ€è¦åœ¨æ—§æ¨¡å‹ä¸Šæ¨ç†ä¸€éï¼Œä»¥è®¡ç®—æŸå¤±å‡½æ•°

            # TODO: è¿™é‡Œçš„æŸå¤±è®¡ç®—å‡½æ•°è¿˜æœ‰ä¸€ç‚¹é—®é¢˜y
            if model.oldModel is not None:
                with torch.no_grad():
                    model.oldModel(image)
            
            y_c_lst = model.curModel.LSC_Classifier(y_output, predict_only=False)
            loss = model.PODloss(true_classes=label.view(-1,1), y_hat_c=y_c_lst)
            loss.backward()
            # è¿™é‡Œåœ¨æ¯ä¸€ä¸ªiterationéƒ½æ›´æ–°ä¸€æ¬¡å­¦ä¹ ç‡
            optimizer.step()
            scheduler.step()

        # å®Œæˆä¸€ä¸ªepochçš„è®­ç»ƒä¹‹åï¼Œåº”è¯¥åœ¨valé›†ä¸Šæ£€éªŒæ€§èƒ½ï¼Œä»¥ä¾¿æ‰§è¡Œæ—©åœ
        # TODO: æ—©åœ 6.1 âˆš
        true_predict = 0
        totol_predict = 0
        with torch.no_grad():
            # for image, label in val_dataloader:
            for image, label in tqdm(val_dataloader, desc=f"evaluating on Epoch {epoch+1}/{max_epoch}",unit="itr",leave=False):
                image = image.to("cuda" if torch.cuda.is_available() else "cpu")
                y_output = model.curModel(image)
                _, result = model.curModel.LSC_Classifier(y_output)
                # æ³¨æ„ï¼Œè¿™é‡Œçš„resultåº”è¯¥æ˜¯æ‰¹é‡çš„å¼ é‡ï¼Œè¦å¯¹å…¶å½¢çŠ¶è¿›è¡Œå…·ä½“çš„åˆ†æ-->å¹¶ä¸æ˜¯,è€Œæ˜¯list[tensors]
                # ä¹‹åå†è¿›è¡Œæ¯”è¾ƒï¼Œå¾—åˆ°é¢„æµ‹æ­£ç¡®çš„æ•°é‡
                # TODO: æ¯”è¾ƒé¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ âˆš
                # resultçš„å½¢çŠ¶åº”è¯¥æ˜¯(batchsize, 1), è€Œä¸”æ˜¯tensor --> 
                result = result.to("cpu").numpy()
                label = label.view(-1,1).numpy()
                true_predict += np.sum(result == label)
                totol_predict += result.shape[0]

        # è¡¥å®Œå¾—å·®ä¸å¤šäº†ï¼Œç»§ç»­åŠªåŠ›ğŸ¥³
        # ç²—ç³™çš„æ—©åœ
        if true_predict/totol_predict < best_acc and epoch > 30:
            print(f"Stop early at Epoch{epoch+1}! Final accuracy: {true_predict/totol_predict:.6f}")
            break
        else:
            best_acc = true_predict/totol_predict if true_predict/totol_predict > best_acc else best_acc
            print(f"acc at epoch {epoch+1}: {best_acc:.6f}")

    # åšæ·±æ‹·è´, å°†å½“å‰å·²è®­ç»ƒå®Œçš„æ¨¡å‹è¿›è¡Œå¤åˆ¶
    model.oldModel = copy.deepcopy(model.curModel)

    pass




'''
if __name__ == '__main__':

    # å®šä¹‰æ•°æ®è½¬æ¢
    # transform = transforms.Compose(
    #     [transforms.ToTensor(),
    #     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    transform = transforms.Compose(
        [transforms.ToTensor()])
    # é€šè¿‡composeæ“ä½œï¼Œå°†datasetå˜æˆ(tensor-->image, int-->class)

    # å¯¼å…¥è®­ç»ƒé›†
    trainset = torchvision.datasets.CIFAR100(root='./dataset', train=True,
                                            download=True, transform=transform)
    Strainset = Subset(trainset, list(range(1000)))
    trainloader = torch.utils.data.DataLoader(Strainset, batch_size=32,
                                            shuffle=True)
    # è¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼Œä¸ºä»€ä¹ˆç”¨ä¸äº†å¤šçº¿ç¨‹

    # ç±»åˆ«æ ‡ç­¾
    # classes = trainset.classes

    # æ‰“å°æ•°æ®é›†å¤§å°
    print('è®­ç»ƒé›†å¤§å°:', len(trainset))
    import cv2
    # cv2.imshow("æ²³ç‹¸",trainset[0][0].numpy().transpose(1,2,0))
    # cv2.waitKey(0)
    # cv2.destroyAllWindows()
    # å¯ä»¥çœ‹å‡ºï¼Œå›¾ç‰‡ç¡®å®å¾ˆå°ï¼Œæ‰€ä»¥è‚¯å®šå¾—åšå›¾ç‰‡ä¸Šçš„å˜æ¢
    for batch_idx,(datas, labels) in enumerate(trainloader):
        # xæ˜¯tensorï¼Œ labelsæ˜¯å­—ç¬¦ä¸²ç±»å‹
        print(datas)
        
        print(labels)

'''

